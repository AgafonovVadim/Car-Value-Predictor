


pip install -U pandas



pip install lxml





import lxml.html as lx
import requests
import numpy as np
import configparser as cp
import pandas as pd
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed





config = cp.ConfigParser() 
config.read("./config.ini")  





all_cars_href = []





# Creating data base

df = pd.DataFrame({
    'full_name': [],
    'status': [],
    'geolocation': [],
    'date': [],
    'year': [],
    'make': [],
    'model_family': [],
    'model_generation': [],
    'model_variant': [],
    'model_trim': [],
    'engine': [],
    'transmission': [],
    'drive_type': [],
    'originality': [],
    'milleage': [],
    'vin': [],
    'vehicle_type': [],
    'body_style': [],
    'doors': [],
    'driver_side': [],
    'ext_color_group': [],
    'int_color_group': [],
    'for_sale': [],
    'avg': [],
    'sales_count': [],
    'dollar_volume': [],
    'lowest_sale': [],
    'top_sale': [],
    'most_recent': [],
})





def download(url: str, page_num: int = 1):
    file_path = f'data/pages/page_{page_num}.html' 
    page = requests.get(url)
    with open(file_path, 'wb') as downloaded_file:
        downloaded_file.write(page.content)


def explore(url: str, page_num: int = 1):
    file_path = f'data/pages/page_{page_num}.html' 
    with open(file_path, 'r') as current_page:
        return lx.fromstring(current_page.read())


def explore_2(url: str):
    global all_cars_href
    all_vehs_href += [config["site"]["main_url"] + href.attrib['href'] for href in lx.fromstring(requests.get(url, verify=False).content).xpath("//a[@class='text-xl leading-5 font-medium table:text-secondary table:text-base flex-1']")]


def download_page(page_info):
    url, page_num = page_info
    download(url, page_num)


def explore_all_pages(url: str):
    download(url)
    root = explore(url)
    pages_count = int(root.xpath("//span[@class='font-medium']")[-1].text)
    pages_count = pages_count // int(config["site"]["auto_count"])
    pages = [url + f'&page={i}' for i in range(pages_count)]

    with ThreadPoolExecutor(multiprocessing.cpu_count()) as executor:
        futures = [executor.submit(explore_2, page) for page in pages]

        # Log progress
        for i, future in enumerate(as_completed(futures), 1):
            try:
                result = future.result()
                if i % 1000 == 0:
                    print(f"Downloaded {i} pages successfully")
            except Exception as e:
                print(f"Error downloading page: {e}")


explore_all_pages('https://www.classic.com/search/')


print(len(all_cars_href))


all_cars_href = []


root = explore(url)
auto_fullnames = root.xpath("//a[@class='text-xl leading-1231313 font-medium table:text-secondary table:text-base flex-1']") 
len(auto_fullnames)
# listing_status = root.xpath("//div[@class='border font-medium uppercase inline-block whitespace-nowrap text-white bg-black border-black px-1 py-0.5 text-sm rounded']")
# listing_date = root.xpath("//span[@class='table:text-black']")
# pages_count = int(root.xpath("//span[@class='font-medium']")[-1].text)
# pages_count = pages_count // 24
# print(pages_count)
# for statuses in pages_count:
#     print(statuses.text)





def download_car(url: str, page_number: int = 0):
    car = requests.get(url)
    file_path = f'data/objects/car_{page_number}.html' 
    with open(file_path, 'wb') as downloaded_file:
        downloaded_file.write(car.content)



download_car('https://www.classic.com/veh/1960-leyland-motors-fv4201-chieftain-tank-323-pEKb2zn/', 30)



parse_car_specs(f'data/objects/car_10.html')


parse_car_prices(f'data/objects/car_17.html')






def parse_car_specs(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()

        tree = lx.fromstring(content)

        # Extract the car title from the title tag
        title = tree.xpath('//title/text()')[0]
        try:
            status = tree.xpath('//div[@class="border font-medium uppercase inline-block whitespace-nowrap text-white bg-black border-black px-2 text-lg rounded"]')[0].text[3:-1]
        except Exception as e:
            try:
                status = tree.xpath('//div[@class="border font-medium uppercase inline-block whitespace-nowrap text-green-600 border-green-600 px-2 text-lg rounded"]')[0].text[3:-1]
            except Exception as e:
                try:
                    status = tree.xpath('//div[@class="border font-medium uppercase inline-block whitespace-nowrap text-red-600 border-red-600 px-2 text-lg rounded"]')[0].text[3:-1]
                except Exception as e:
                    status = tree.xpath('//div[@class="border font-medium uppercase inline-block whitespace-nowrap text-black bg-gray-200 px-2 text-lg rounded"]')[0].text[3:-1]
        
        try:
            date = tree.xpath('//div[@class="flex text-gray-500 pl-2 ml-auto "]')[0].text[11:-9]
        except Exception as e:
            date = tree.xpath('//div[@class="flex text-gray-500 pl-2 ml-auto md:hidden"]')[0].text[11:-9]
        geolocation = tree.xpath("//div[@class='flex gap-2 items-center text-black md:text-white']/div")[0].text
        # Extract all divs with class "font-medium"
        specs_divs = tree.xpath('//div[@class="font-medium"]')

        # Collect specs from each div
        specs = [div.text_content().strip() for div in specs_divs]
        if (len(specs)) > 18:
            specs = specs[:(len(specs) // 2)]
        if len(specs) != 18:
            specs.insert(5, None)
        for i in range(len(specs)):
            if specs[i] == '-' or specs[i] == '' or specs[i] == 'N/A':
                specs[i] = None
            if specs[i] is not None and specs[i].find('mi') != -1:
                specs[i] = tree.xpath("//div[@class='flex gap-2 items-center text-black md:text-white']/div")[1].text
                specs[i] = specs[i][specs[i].find('(') + 1:specs[i].find('mi') - 1]

        car_specs = [title[:title.find('VIN') - 1], status, geolocation, date] + specs

        return car_specs

    except Exception as e:
        return []


def download_prices(url: str):
    prices = requests.get(url)
    file_path = f'data/objects/car_prices.html' 
    with open(file_path, 'wb') as downloaded_file:
        downloaded_file.write(prices.content)


def parse_car_prices(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read()
        tree = lx.fromstring(content)
        a_link = tree.xpath('//a[@class="typography-body1 flex items-center text-sm hover:bg-blue-100/30 p-2 rounded cursor-pointer transition duration-300 ease-in-out"]')
        download_prices(main_url + a_link[0].attrib['href'])
        with open('data/objects/car_prices.html', 'r', encoding='utf-8') as file:
            content = file.read()
        tree = lx.fromstring(content)
        try:
            prices_span = tree.xpath('//span[@class="md:text-center font-medium md:font-light md:text-2xl"]')
            if len(prices_span) != 0:
                prices_span = tree.xpath('//span[@class="md:text-center font-medium md:font-light md:text-2xl"]')
                prices = [span.text_content().strip().replace(',', '.') for span in prices_span]
                for i in range(len(prices)):
                    if prices[i][0] == '$':
                        prices[i] = prices[i][1:]
                        if prices[i][-1] == 'm':
                            prices[i] = int(float(prices[i][:-1]) * 1000000)
                        elif prices[i].find('.') != -1:
                            prices[i] = int(float(prices[i]) * 1000)
                    if prices[i] == '-' or prices[i] == '' or prices[i] == 'N/A':
                        prices[i] = None
                return prices 
            else:
                prices_span = tree.xpath('//span[@class="text-2xl font-medium"]')
                name_span = tree.xpath('//span[@class="text-xs text-gray-500 font-medium"]')
                prices = ["-"] * 7
                for i in range(len(name_span)):
                    if name_span[i].text == "FOR SALE" or name_span[i].text == "LISTINGS":
                        prices[0] = prices_span[i].text[19:-17]
                    elif name_span[i].text == "HIGHEST SALE" or name_span[i].text == "TOP SALE":
                        prices[5] = prices_span[i].text[19:-17].replace(',', '.')
                    elif name_span[i].text == "DOLLAR VOLUME":
                        prices[3] = prices_span[i].text[19:-17]
                for i in range(len(prices)):
                    if prices[i][0] == '$':
                        prices[i] = prices[i][1:]
                        if prices[i][-1] == 'm':
                            prices[i] = int(float(prices[i][:-1]) * 1000000)
                        elif prices[i].find('.') != -1:
                            prices[i] = int(float(prices[i]) * 1000)
                    if prices[i] == '-' or prices[i] == '' or prices[i] == 'N/A':
                        prices[i] = None
                return prices 
        except Exception as e:
            return "kek2"
    except Exception as e:
        return "kek"


# Full parsing


# Analysing number of pages
download(url)
root = explore(url)
pages_count = int(root.xpath("//span[@class='font-medium']")[-1].text)
pages_count = pages_count // 24
for i in range(pages_count):
    

print(pages_count)


#Check checl

root = explore(url)
#pages_count = root.xpath("//div[@class='flex gap-2 items-center text-white']/div")
pages_count = root.xpath("//div[@class='flex gap-2 items-center text-black md:text-white']/div")
divs = pages_count[1].text






#Check check specs
url = 'https://www.classic.com/veh/1914-buick-b-24-11468-WN2MGAW/'
url_1 = 'https://www.classic.com/veh/1928-indian-four-153-4AwvxbW/'
url_2 = 'https://www.classic.com/veh/1984-bmw-733i-wbaff840xe9284528-Wvkg85p/'
url_3 = 'https://www.classic.com/veh/1984-toyota-land-cruiser-jt3fj60g4e1119893-4AjRozn/'
url_4 = 'https://www.classic.com/veh/2006-subaru-impreza-wrx-sti-6zz60000gdb037755-pPEo7R4/'

df = pd.concat([df, pd.DataFrame([parse_car_specs(f'data/pages/page.html') + parse_car_prices(f'data/pages/page.html')], columns=df.columns)], ignore_index=True)
print(df)


#Check check prices
url = 'https://www.classic.com/veh/1960-leyland-motors-fv4201-chieftain-tank-323-pEKb2zn'
url_1 = 'https://www.classic.com/veh/1928-indian-four-153-4AwvxbW/'
url_2 = 'https://www.classic.com/veh/1984-bmw-733i-wbaff840xe9284528-Wvkg85p/'
url_3 = 'https://www.classic.com/veh/1984-toyota-land-cruiser-jt3fj60g4e1119893-4AjRozn/'
url_4 = 'https://www.classic.com/veh/2006-subaru-impreza-wrx-sti-6zz60000gdb037755-pPEo7R4/'
download(url_4)
parse_car_prices(f'data/pages/page.html')


url_1 = 'https://www.classic.com/veh/1928-indian-four-153-4AwvxbW/'
download(url_1)
parse_car_specs(f'data/pages/page.html') + parse_car_prices(f'data/pages/page.html')


parse_car_prices(f'data/pages/page.html')


["-" ] * 7



